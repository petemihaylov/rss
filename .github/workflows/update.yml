name: Fetch Dotnet Articles

on:
  schedule:
    - cron: '0 16 * * *'  # Runs daily at 16:00 UTC
  workflow_dispatch:

jobs:
  fetch:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pytz feedparser beautifulsoup4
      
      - name: Update the README
        run: |
          python - <<EOF
          import requests
          import feedparser
          import random
          from datetime import datetime
          import pytz
          from bs4 import BeautifulSoup

          # List of topics
          TOPICS = [".NET", "C#", "Azure", "Blazor", "Entity Framework", "ASP.NET Core", "Microservices"]

          def get_random_topic():
              return random.choice(TOPICS)

          def fetch_microsoft_devblogs():
              """Fetches the latest article from Microsoft DevBlogs"""
              url = "https://devblogs.microsoft.com/dotnet/feed/"
              feed = feedparser.parse(url)

              if not feed.entries:
                  return None

              article = feed.entries[0]
              return f"- [{article.title}]({article.link}) by Microsoft DevBlogs"

          def fetch_medium_search(topic):
              """Fetches an article from Medium based on the topic"""
              search_query = topic.replace(" ", "+")
              url = f"https://medium.com/search?q={search_query}"
              headers = {"User-Agent": "Mozilla/5.0"}
              response = requests.get(url, headers=headers)

              if response.status_code != 200:
                  return None

              soup = BeautifulSoup(response.text, "html.parser")
              articles = soup.select("h2")  # Find article titles

              if not articles:
                  return None

              title = articles[0].text
              link_tag = soup.select("a[href*='/@']")

              if link_tag:
                  link = "https://medium.com" + link_tag[0]["href"]
              else:
                  link = "No link found."

              return f"- [{title}]({link}) from Medium"

          def update_readme():
              """Updates the README with fresh content"""
              
              topic = get_random_topic()
              print(f"Selected topic: {topic}")

              article = fetch_microsoft_devblogs() or fetch_medium_search(topic) or "No articles found."

              date_str = datetime.now(pytz.UTC).strftime('%Y-%m-%d %H:%M:%S UTC')

              new_content = f"{article}\n"

              with open("README.md", "w") as file:
                  file.write(new_content)

              print("README updated successfully.")

          if __name__ == "__main__":
              try:
                  update_readme()
              except Exception as e:
                  print(f"Error occurred: {str(e)}")
                  exit(1)
          EOF
      
      - name: Commit and Push changes
        run: |
          git config --global user.name "${{ secrets.GIT_USER_NAME }}"
          git config --global user.email "${{ secrets.GIT_USER_EMAIL }}"
          
          git add README.md
          
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "Daily dev news update: $(date '+%Y-%m-%d')"
            git push
          fi
